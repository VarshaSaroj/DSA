Algorithmic Techniques: Search techniques (backtracking and bounding)
Backtracking and bounding are two types of algorithmic techniques for solving problems that involve searching for solutions in a large space of possibilities12. Backtracking is used to find some or all solutions to a problem, while bounding is used to find the optimal solution to a problem1. Both techniques use a state space tree to represent the possible solutions and explore them in a depth-first manner23. However, they differ in how they prune the tree and reduce the search space. Backtracking uses a constraint satisfaction approach, which means it discards any partial solution that does not satisfy the given constraints4. Bounding uses an optimization approach, which means it discards any partial solution that has a lower or higher value than the best solution found so far, depending on whether the problem is a minimization or maximization problem2. You can learn more about these techniques and see some examples from GeeksforGeeks435. Do you have any questions?
Time and space complexity, average and worst-case analysis, asymptotic notation, recurrence equations and their solution.

Time and space complexity are two important measures of an algorithm’s performance. They indicate how much time and memory the algorithm requires to run as a function of the input size12. Average and worst-case analysis are two ways of estimating the time complexity of an algorithm. They consider the average or worst possible input for the algorithm and calculate how many steps the algorithm takes to complete1. Asymptotic notation is a way of expressing the time complexity of an algorithm in a simplified form, ignoring constant factors and lower-order terms. The most common asymptotic notations are big O, big Omega, and big Theta, which represent the upper bound, lower bound, and exact bound of the time complexity, respectively1. Recurrence equations are equations that describe how the time complexity of an algorithm depends on the time complexity of its subproblems. They are often used to analyze algorithms that use divide and conquer or recursion techniques. To solve recurrence equations, we can use methods such as substitution, recursion tree, or master theorem1. You can find more details and examples of these concepts from HackerEarth1 or GeeksforGeeks34. Does this answer your question?
Search Trees, Sorting algorithms - heapsort, quick sort, sorting in linear time (counting sort, radix sort, bucket sort),

Search trees and sorting algorithms are two related topics in computer science. A search tree is a data structure that allows efficient searching, insertion, and deletion of elements. A sorting algorithm is a method of arranging elements in a certain order, such as ascending or descending. There are many types of search trees and sorting algorithms, each with different advantages and disadvantages1.
One example of a sorting algorithm that uses a search tree is tree sort. Tree sort is an online sorting algorithm that builds a binary search tree from the elements to be sorted, and then traverses the tree in in-order, so that the elements come out in sorted order234. Tree sort has a time complexity of O(n log n) in the average case, but O(n^2) in the worst case when the tree becomes unbalanced4.
Some other examples of sorting algorithms are heap sort, quick sort, counting sort, radix sort, and bucket sort. Heap sort uses a binary heap to sort elements in O(n log n) time in the worst case1. Quick sort uses a partitioning technique to divide and conquer the elements in O(n log n) time in the average case, but O(n^2) time in the worst case when the pivot is poorly chosen1. Counting sort, radix sort, and bucket sort are examples of non-comparison sorting algorithms that can sort elements in O(n) time in the best case, but require some assumptions about the input range or distribution1. You can learn more about these algorithms and see some implementations from GeeksforGeeks21 or Baeldung on Computer Science35. Is there anything else you want to know?
Greedy algorithms (Activity-selection problem, Huffman coding, knapsack, shortest path and minimum spanning tree in graphs)

Greedy algorithms are a type of algorithm that makes the locally optimal choice at each step, hoping to find the globally optimal solution. Greedy algorithms are often simple, fast, and easy to implement, but they do not always guarantee the optimal solution123. Greedy algorithms work well when the problem has the optimal substructure and greedy choice properties. Optimal substructure means that the optimal solution of the problem can be obtained by using the optimal solutions of its subproblems. Greedy choice means that the locally optimal choice leads to the globally optimal solution423.
Some examples of problems that can be solved using greedy algorithms are:
•	Activity-selection problem: Given a set of activities, each with a start and finish time, find the maximum number of activities that can be performed by a single person or resource, assuming that a person can only work on a single activity at a time. The greedy choice is to pick the activity that finishes earliest, and then recursively solve the subproblem of the remaining activities that start after the chosen activity43.
•	Huffman coding: Given a set of symbols, each with a frequency or weight, find a variable-length code for each symbol that minimizes the average length of the code. The greedy choice is to assign shorter codes to more frequent symbols, and longer codes to less frequent symbols. This can be done by building a binary tree where each node has a weight equal to the sum of its children’s weights, and each leaf represents a symbol. The greedy choice is to merge the two nodes with the smallest weights until there is only one node left123.
•	Knapsack problem: Given a set of items, each with a weight and a value, find the subset of items that maximizes the total value while staying within a given weight limit. The greedy choice is to pick the item with the highest value-to-weight ratio, and then recursively solve the subproblem of the remaining items and weight limit. However, this greedy choice does not always lead to the optimal solution, as it may ignore some items that have lower value-to-weight ratios but higher values43.
•	Shortest path problem: Given a weighted graph, find the shortest path from a source node to a destination node. The greedy choice is to pick the edge with the smallest weight that leads closer to the destination node, and then recursively solve the subproblem of the remaining graph and destination node. However, this greedy choice does not always lead to the optimal solution, as it may ignore some edges that have higher weights but lead to shorter paths43.
•	Minimum spanning tree problem: Given a weighted graph, find a subset of edges that connects all nodes with minimum total weight. The greedy choice is to pick the edge with the smallest weight that does not form a cycle with the already chosen edges, and then recursively solve the subproblem of the remaining graph. This greedy choice always leads to the optimal solution, as proven by Kruskal’s algorithm and Prim’s algorithm143.
You can find more details and examples of these problems and their greedy solutions from Wikipedia1, GeeksforGeeks45, Simplilearn2, or freeCodeCamp36. Do you have any questions?
Divide and conquer - Merge Sort, Integer Multiplication, Solving Recurrence-substitution method and recursion-tree, master theorem

Divide and conquer is a technique that breaks a problem into smaller and simpler subproblems, solves them recursively, and combines their solutions to get the solution of the original problem123. Divide and conquer works well when the problem has the optimal substructure property, which means that the optimal solution of the problem can be obtained by using the optimal solutions of its subproblems13.
Some examples of problems that can be solved using divide and conquer are:
•	Merge sort: Given an array of elements, sort them in a certain order, such as ascending or descending. The divide step is to split the array into two halves, the conquer step is to sort the two halves recursively using merge sort, and the combine step is to merge the two sorted halves into one sorted array123.
•	Integer multiplication: Given two large integers, multiply them efficiently. The divide step is to split each integer into two halves, the conquer step is to multiply the four halves recursively using integer multiplication, and the combine step is to add the four products using appropriate shifts and carries13.
•	Solving recurrence: Given a recurrence relation that describes the time complexity of an algorithm, find a closed-form expression or an asymptotic bound for it. The divide step is to express the recurrence in terms of smaller subproblems, the conquer step is to solve the smaller subproblems recursively using substitution method, recursion-tree method, or master theorem, and the combine step is to simplify the expression or find the dominant term13.
You can find more details and examples of these problems and their divide and conquer solutions from GeeksforGeeks1, Wisdom Answer2, or Programiz3. Do you have any questions?
Dynamic programming (0/1 knapsack, Traveling salesman problem, matrix multiplication, all-pairs shortest paths, longest common subsequence, optimal binary search trees)

Dynamic programming is a technique that breaks a problem into smaller and overlapping subproblems, solves them once and stores their solutions in a table, and reuses their solutions to get the solution of the original problem123. Dynamic programming works well when the problem has the optimal substructure and overlapping subproblems properties. Optimal substructure means that the optimal solution of the problem can be obtained by using the optimal solutions of its subproblems. Overlapping subproblems means that the same subproblems are solved repeatedly123.
Some examples of problems that can be solved using dynamic programming are:
•	0/1 knapsack problem: Given a set of items, each with a weight and a value, find the subset of items that maximizes the total value while staying within a given weight limit. The subproblems are to find the optimal value for smaller subsets of items and weight limits. The table stores the optimal values for each subproblem. The solution is obtained by looking up the table for the original problem14.
•	Traveling salesman problem: Given a set of cities and distances between them, find the shortest tour that visits each city exactly once and returns to the starting city. The subproblems are to find the shortest tour for smaller subsets of cities and starting points. The table stores the shortest distances for each subproblem. The solution is obtained by adding the distances for each city in the tour13.
•	Matrix multiplication: Given a sequence of matrices, find the most efficient way to multiply them together. The cost of multiplying two matrices is proportional to their dimensions. The subproblems are to find the minimum cost for smaller subsequences of matrices. The table stores the minimum costs for each subproblem. The solution is obtained by looking up the table for the original problem123.
•	All-pairs shortest paths: Given a weighted graph, find the shortest path between every pair of nodes. The subproblems are to find the shortest path between every pair of nodes using only a subset of intermediate nodes. The table stores the shortest distances for each subproblem. The solution is obtained by looking up the table for the original problem13.
•	Longest common subsequence: Given two sequences, find the longest subsequence that is common to both of them. A subsequence is a sequence that can be derived from another sequence by deleting some elements without changing the order of the remaining elements. The subproblems are to find the longest common subsequence for smaller prefixes of both sequences. The table stores the lengths of the longest common subsequences for each subproblem. The solution is obtained by tracing back the table from the original problem123.
•	Optimal binary search trees: Given a set of keys, each with a frequency or probability of being searched, find a binary search tree that minimizes the average search cost. The cost of searching a key is proportional to its depth in the tree. The subproblems are to find the minimum cost for smaller subsets of keys. The table stores the minimum costs and roots for each subproblem. The solution is obtained by constructing the tree from the roots stored in the table13.
You can find more details and examples of these problems and their dynamic programming solutions from Tutorials Point1, Programiz2, GeeksforGeeks3, or MakeUseOf4. Do you have any questions?
Computational complexity: Problem classes: P, NP, NP-complete, NP-hard. Reduction. Examples of NP-complete problems.

Computational complexity is the study of how much resources, such as time or memory, are required to solve a computational problem1234. Computational complexity theory classifies problems into different sets, called complexity classes, based on their difficulty and the type of computation needed to solve them1234.
Some of the most important complexity classes are:
•	P: The class of problems that can be solved by a deterministic Turing machine in polynomial time. A deterministic Turing machine is a model of computation that has a finite set of states and a tape that can be read and written. Polynomial time means that the number of steps required to solve the problem is bounded by some polynomial function of the size of the input1234.
•	NP: The class of problems that can be solved by a nondeterministic Turing machine in polynomial time. A nondeterministic Turing machine is a model of computation that has a finite set of states and a tape that can be read and written, but can also make choices at each step. Polynomial time means that the number of steps required to solve the problem is bounded by some polynomial function of the size of the input1234.
•	NP-complete: The class of problems that are in NP and are also the hardest problems in NP. A problem is NP-complete if every other problem in NP can be reduced to it in polynomial time. A reduction is a way of transforming one problem into another problem such that solving the second problem also solves the first problem1234.
•	NP-hard: The class of problems that are at least as hard as the NP-complete problems. A problem is NP-hard if every problem in NP can be reduced to it in polynomial time. NP-hard problems may or may not be in NP1234.
Some examples of NP-complete problems are:
•	Satisfiability problem (SAT): Given a Boolean formula with variables and logical operators, find an assignment of truth values to the variables that makes the formula true, or determine that no such assignment exists1234.
•	Traveling salesman problem (TSP): Given a set of cities and distances between them, find the shortest tour that visits each city exactly once and returns to the starting city, or determine that no such tour exists1234.
•	Knapsack problem: Given a set of items, each with a weight and a value, find the subset of items that maximizes the total value while staying within a given weight limit, or determine that no such subset exists[^1





